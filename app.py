# app.py ‚Äî 2-class (spam / not_spam) + server-side filters + UTF-8 IO
# + live dashboard + refresh + Gmail totals + BULK CLASSIFY ENTIRE MAILBOX
# + DB-backed email browser with filters & pagination
# + Spam/Non-Spam doughnut chart data on Dashboard & Reports (unified)
from flask import (
    Flask, redirect, url_for, session, request, render_template,
    Response, flash, make_response, send_file, jsonify
)
from flask_session import Session
import sqlite3
import os
import csv
import json
import io
from io import StringIO, BytesIO
import joblib
import pandas as pd
from datetime import datetime, date, timedelta

from google_auth_oauthlib.flow import Flow
import google.auth.transport.requests
from google.oauth2 import id_token
from google.oauth2.credentials import Credentials
from google.auth.exceptions import RefreshError

# ------------------- Flask Setup -------------------
app = Flask(__name__)
app.secret_key = "your_secret_key_here"
app.config["SESSION_TYPE"] = "filesystem"
Session(app)

os.environ["OAUTHLIB_INSECURE_TRANSPORT"] = "1"
GOOGLE_CLIENT_ID = "388546658899-3o5mh5sishffm821gmgegkj984ghvsc4.apps.googleusercontent.com"

# ------------------- Paths -------------------
MODEL_PATH = "email_classifier.pkl"
VEC_PATH  = "vectorizer.pkl"
INFO_PATH = "model_info.txt"
MASTER_DATASET = "spam_assassin.csv"   # merged + dedup master
HISTORY_PATH = "model_history.csv"
DB_PATH = "users.db"
DATASET_LOG_PATH = "dataset_history.csv"   # <- used by logs page & upload logger

# ------------------- App uptime -------------------
START_TIME = datetime.now()
def _uptime_str():
    d = datetime.now() - START_TIME
    days = d.days
    h, r = divmod(d.seconds, 3600)
    m, _ = divmod(r, 60)
    return f"{days}d {h}h {m}m" if days else f"{h}h {m}m"

# ------------------- Load Model -------------------
model, vectorizer = None, None
if os.path.exists(MODEL_PATH) and os.path.exists(VEC_PATH):
    model = joblib.load(MODEL_PATH)
    vectorizer = joblib.load(VEC_PATH)

# ------------------- DB Bootstrap -------------------
def db_connect():
    return sqlite3.connect(DB_PATH, check_same_thread=False)

def ensure_schema():
    con = db_connect()
    cur = con.cursor()
    # Users table
    cur.execute("""
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            email TEXT UNIQUE,
            role TEXT
        )
    """)
    # Filtered emails (dedupe by gmail_id)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS filtered_emails (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_email TEXT,
            gmail_id TEXT UNIQUE,
            sender TEXT,
            subject TEXT,
            snippet TEXT,
            classification TEXT,   -- 'spam' | 'not_spam' (legacy 'phishing' may exist)
            created_at TEXT
        )
    """)
    # Backfill columns / indexes if DB existed before
    cur.execute("PRAGMA table_info(filtered_emails)")
    cols = {r[1] for r in cur.fetchall()}
    if "gmail_id" not in cols:
        cur.execute("ALTER TABLE filtered_emails ADD COLUMN gmail_id TEXT")
    if "created_at" not in cols:
        cur.execute("ALTER TABLE filtered_emails ADD COLUMN created_at TEXT")
        cur.execute("UPDATE filtered_emails SET created_at = strftime('%Y-%m-%d %H:%M:%S','now') WHERE created_at IS NULL")
    cur.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_filtered_emails_gmail_id ON filtered_emails(gmail_id)")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_filtered_user ON filtered_emails(user_email)")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_filtered_cls ON filtered_emails(classification)")
    cur.execute("CREATE INDEX IF NOT EXISTS idx_filtered_created ON filtered_emails(created_at)")
    con.commit()
    con.close()
ensure_schema()

# ------------------- Helpers -------------------
def _norm_label_raw(s: str) -> str:
    """Normalize raw labels from model or legacy DB rows into a small set."""
    s = (s or "").strip().lower().replace("-", "_")
    if s in {"ham", "non_spam", "not spam", "not_spam"}:
        return "not_spam"
    if s in {"phish", "phishing", "spam"}:
        return "spam"  # üëà collapse phishing -> spam
    return "not_spam"

def classify_email(body: str = "", subject: str = "", snippet: str = "") -> str:
    """Run model; always return only 'spam' or 'not_spam'."""
    text = " ".join([(subject or ""), (snippet or ""), (body or "")]).strip()
    if not text or model is None or vectorizer is None:
        return "not_spam"
    label = model.predict(vectorizer.transform([text]))[0]
    return _norm_label_raw(label)

def get_redirect_uri():
    return f"{request.scheme}://{request.host}/callback"

def refresh_google_token_if_needed(creds):
    """Refresh Google token if expired."""
    try:
        creds.refresh(google.auth.transport.requests.Request())
        return creds
    except RefreshError:
        session.clear()
        flash("‚ö†Ô∏è Google session expired. Please log in again.", "warning")
        return None

def save_filtered_email(user_email, gmail_id, sender, subject, snippet, classification):
    """Insert if not exists (dedupe by gmail_id). Collapse to binary before save."""
    classification = _norm_label_raw(classification)
    con = db_connect()
    cur = con.cursor()
    try:
        cur.execute("""
            INSERT OR IGNORE INTO filtered_emails
            (user_email, gmail_id, sender, subject, snippet, classification, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            user_email, gmail_id, sender, subject, snippet, classification,
            datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        ))
        con.commit()
    finally:
        con.close()

# ---- Dataset log helper (CSV) ----
def append_dataset_log(filename: str, rows: int, cols: int, detected_format: str | None):
    detected_format = detected_format or "Unknown"
    exists = os.path.exists(DATASET_LOG_PATH)
    with open(DATASET_LOG_PATH, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(
            f, fieldnames=["filename", "rows", "columns", "detected_format", "timestamp"]
        )
        if not exists:
            writer.writeheader()
        writer.writerow({
            "filename": filename,
            "rows": int(rows or 0),
            "columns": int(cols or 0),
            "detected_format": detected_format,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        })

def get_gmail_totals():
    """Returns: dict(messages_total, threads_total, inbox_total, spam_total)."""
    totals = {"messages_total": 0, "threads_total": 0, "inbox_total": 0, "spam_total": 0}
    try:
        if "credentials" not in session:
            return totals
        creds = Credentials(**session["credentials"])
        creds = refresh_google_token_if_needed(creds)
        if not creds:
            return totals
        from googleapiclient.discovery import build
        svc = build("gmail", "v1", credentials=creds)
        prof = svc.users().getProfile(userId="me").execute()
        totals["messages_total"] = int(prof.get("messagesTotal", 0))
        totals["threads_total"]  = int(prof.get("threadsTotal", 0))
        inbox = svc.users().labels().get(userId="me", id="INBOX").execute()
        spam  = svc.users().labels().get(userId="me", id="SPAM").execute()
        totals["inbox_total"] = int(inbox.get("messagesTotal", 0))
        totals["spam_total"]  = int(spam.get("messagesTotal", 0))
    except Exception:
        pass
    return totals

def _read_model_metrics():
    """
    Parse INFO_PATH for accuracy & macro metrics written by train_model.py.
    Returns strings ready for display.
    """
    out = {
        "accuracy": "N/A",
        "precision": "N/A",
        "recall": "N/A",
        "f1": "N/A",
        "trained_on": "N/A",
    }
    if not os.path.exists(INFO_PATH):
        return out

    macro_p = macro_r = macro_f = None
    metrics_json = None
    with open(INFO_PATH, encoding="utf-8") as f:
        for raw in f:
            line = raw.strip()
            if line.startswith("accuracy="):
                out["accuracy"] = line.split("=", 1)[1].strip()
            elif line.startswith("trained_on="):
                out["trained_on"] = line.split("=", 1)[1].strip()
            elif line.startswith("macro_precision="):
                macro_p = line.split("=", 1)[1].strip()
            elif line.startswith("macro_recall="):
                macro_r = line.split("=", 1)[1].strip()
            elif line.startswith("macro_f1="):
                macro_f = line.split("=", 1)[1].strip()
            elif line.startswith("metrics_json="):
                try:
                    metrics_json = json.loads(line.split("=", 1)[1])
                except Exception:
                    metrics_json = None

    # Prefer explicit macro_* lines; else fallback to metrics_json ('macro avg' keys are 0..1 scale)
    if macro_p and macro_r and macro_f:
        out["precision"] = macro_p
        out["recall"] = macro_r
        out["f1"] = macro_f
    elif metrics_json and "macro avg" in metrics_json:
        m = metrics_json["macro avg"]
        to_pct = lambda v: f"{(float(v)*100):.2f}" if isinstance(v, (int, float)) or str(v).replace(".","",1).isdigit() else "N/A"
        out["precision"] = to_pct(m.get("precision", 0))
        out["recall"] = to_pct(m.get("recall", 0))
        out["f1"] = to_pct(m.get("f1-score", 0))

    return out

def _spam_nonspam_counts(limit_to_email: str | None):
    """
    Return tuple (spam_count, not_spam_count) for either:
      - a given user (limit_to_email=str)
      - or all users (limit_to_email=None)
    """
    con = db_connect()
    cur = con.cursor()
    if limit_to_email:
        where_sql = "WHERE user_email=?"
        params = (limit_to_email,)
    else:
        where_sql = ""
        params = ()
    cur.execute(f"""
        SELECT
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END) AS spam_cnt,
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END) AS not_spam_cnt
        FROM filtered_emails
        {where_sql}
    """, params)
    row = cur.fetchone() or (0, 0)
    con.close()
    return int(row[0] or 0), int(row[1] or 0)

# ---- Range-based helpers (for Reports) ----
def _parse_date(s: str, default: date) -> date:
    try:
        return datetime.strptime(s, "%Y-%m-%d").date()
    except Exception:
        return default

def _range_counts(start_d: date, end_d: date, limit_to_email: str | None = None):
    """
    Count spam/not_spam for a date window (inclusive), optionally for one user.
    created_at is stored as 'YYYY-MM-DD HH:MM:SS'.
    """
    con = db_connect()
    cur = con.cursor()
    where = ["DATE(substr(created_at,1,10)) BETWEEN ? AND ?"]
    params = [start_d.isoformat(), end_d.isoformat()]
    if limit_to_email:
        where.append("user_email=?")
        params.append(limit_to_email)

    cur.execute(f"""
        SELECT
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END),
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END)
        FROM filtered_emails
        WHERE {' AND '.join(where)}
    """, params)
    spam_cnt, notspam_cnt = cur.fetchone() or (0, 0)

    # User list in window
    cur.execute(f"""
        SELECT DISTINCT user_email FROM filtered_emails
        WHERE {' AND '.join(where)}
        AND user_email IS NOT NULL AND user_email <> ''
    """, params)
    users = sorted([r[0] for r in cur.fetchall()])

    con.close()
    return int(spam_cnt or 0), int(notspam_cnt or 0), users

# ---------- SINGLE SOURCE OF TRUTH for Reports + CSV ----------
def build_reports_summary():
    """Compute the same numbers used in the Reports page & the exports."""
    con = db_connect()
    c = con.cursor()

    c.execute("SELECT COUNT(*) FROM filtered_emails")
    total_emails = (c.fetchone() or (0,))[0]

    # spam includes legacy 'phishing'
    c.execute("""
      SELECT
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END) AS spam_cnt,
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END) AS nspam_cnt
      FROM filtered_emails
    """)
    spam_emails, not_spam_emails = c.fetchone() or (0, 0)

    c.execute("SELECT COUNT(DISTINCT user_email) FROM filtered_emails")
    active_users = c.fetchone()[0] or 0
    con.close()

    metrics = _read_model_metrics()

    summary = {
        # distribution
        "emails_processed": int(total_emails or 0),
        "spam_detected": int(spam_emails or 0),
        "non_spam": int(not_spam_emails or 0),

        # model metrics
        "accuracy": metrics["accuracy"],
        "precision": metrics["precision"],
        "recall": metrics["recall"],
        "f1": metrics["f1"],
        "trained_on": metrics["trained_on"],

        # system + activity
        "uptime": _uptime_str(),
        "avg_response": "247ms",
        "memory_usage": "68%",
        "cpu_usage": "34%",
        "active_users": int(active_users or 0),
        "emails_flagged": int(spam_emails or 0),
        "training_sessions": 3,
        "false_positives": 12,
        "false_negatives": 8,
        "admin_actions": 0,
    }
    return summary

# ---- Username helper (prevents KeyError when 'email' is missing) ----
def current_username():
    name = session.get("name")
    if name:
        return name
    email = session.get("email")
    if email:
        return email.split("@")[0].title()
    return "Guest"

# ------------------- Routes -------------------
@app.route("/")
def index():
    if "email" in session:
        return redirect(url_for("dashboard"))
    return render_template("index.html")

# ------------------- Google Login -------------------
@app.route("/login")
def login():
    session.pop("credentials", None)
    flow = Flow.from_client_secrets_file(
        "client_secret.json",
        scopes=[
            "https://www.googleapis.com/auth/userinfo.email",
            "https://www.googleapis.com/auth/userinfo.profile",
            "openid",
            "https://www.googleapis.com/auth/gmail.readonly",
        ],
        redirect_uri=get_redirect_uri(),
    )
    authorization_url, state = flow.authorization_url(
        access_type="offline", include_granted_scopes="true", prompt="consent"
    )
    session["state"] = state
    return redirect(authorization_url)

# ------------------- OAuth Callback -------------------
@app.route("/callback")
def callback():
    flow = Flow.from_client_secrets_file(
        "client_secret.json",
        scopes=[
            "https://www.googleapis.com/auth/userinfo.email",
            "https://www.googleapis.com/auth/userinfo.profile",
            "openid",
            "https://www.googleapis.com/auth/gmail.readonly",
        ],
        redirect_uri=get_redirect_uri(),
    )
    flow.fetch_token(authorization_response=request.url)
    credentials = flow.credentials

    token_request = google.auth.transport.requests.Request()
    id_info = id_token.verify_oauth2_token(credentials.id_token, token_request, GOOGLE_CLIENT_ID)
    email = id_info.get("email")
    name = id_info.get("name", email.split("@")[0])

    # ‚úÖ User setup
    conn = db_connect()
    c = conn.cursor()
    c.execute("SELECT role FROM users WHERE email = ?", (email,))
    row = c.fetchone()
    role = row[0] if row else "user"
    if not row:
        c.execute("INSERT INTO users (email, role) VALUES (?, ?)", (email, role))
        conn.commit()
    conn.close()

    # ‚úÖ Session store
    session["email"] = email
    session["role"] = role
    session["name"] = name
    session["credentials"] = {
        "token": credentials.token,
        "refresh_token": credentials.refresh_token,
        "token_uri": credentials.token_uri,
        "client_id": credentials.client_id,
        "client_secret": credentials.client_secret,
        "scopes": credentials.scopes,
    }

    flash("‚úÖ Logged in successfully!", "success")
    return redirect(url_for("dashboard"))

# ------------------- Dashboard (live) -------------------
@app.route("/dashboard")
def dashboard():
    if "email" not in session:
        return redirect(url_for("index"))

    # Model info (from model_info.txt)
    metrics = _read_model_metrics()
    accuracy = metrics["accuracy"]
    precision = metrics["precision"]
    recall = metrics["recall"]
    f1 = metrics["f1"]
    trained_on = metrics["trained_on"]

    # Total training data = merged master rows (all datasets combined)
    samples = 0
    if os.path.exists(MASTER_DATASET):
        try:
            samples = len(pd.read_csv(MASTER_DATASET, encoding="utf-8"))
        except UnicodeDecodeError:
            samples = len(pd.read_csv(MASTER_DATASET, encoding="latin-1"))

    con = db_connect(); cur = con.cursor()
    is_admin = (session.get("role") == "admin")

    # WHERE helpers
    where_today = "WHERE DATE(created_at)=DATE('now','localtime')" if is_admin \
        else "WHERE user_email=? AND DATE(created_at)=DATE('now','localtime')"
    params_today = () if is_admin else (session.get("email"),)

    where_all = "" if is_admin else "WHERE user_email=?"
    params_all = () if is_admin else (session.get("email"),)

    # Today counts (binary; treat legacy 'phishing' as spam)
    cur.execute(f"""
      SELECT
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END) AS spam_cnt,
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END) AS not_spam_cnt,
        COUNT(*)
      FROM filtered_emails
      {where_today}
    """, params_today)
    spam_t, not_spam_t, total_t = cur.fetchone() or (0, 0, 0)

    # All-time totals
    cur.execute(f"""
      SELECT
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END) AS spam_cnt,
        SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END) AS not_spam_cnt,
        COUNT(*)
      FROM filtered_emails
      {where_all}
    """, params_all)
    spam_all, not_spam_all, total_all = cur.fetchone() or (0, 0, 0)
    con.close()

    # Percentages (all-time filtered)
    total_for_pct = (spam_all + not_spam_all) or 1
    spam_pct = round(100 * (spam_all) / total_for_pct)
    nonspam_pct = 100 - spam_pct

    gmail_totals = get_gmail_totals()
    username = current_username()

    # üëá Provide BOTH sets of keys so templates can branch cleanly by role
    data = {
        "username": username,
        "role": session.get("role"),
        "is_admin": is_admin,

        "emails_filtered_today": int(total_t or 0),
        "emails_filtered_total": int(total_all or 0),
        "emails_gmail_total": gmail_totals.get("messages_total", 0),
        "emails_gmail_inbox": gmail_totals.get("inbox_total", 0),
        "emails_gmail_spam": gmail_totals.get("spam_total", 0),

        "spam_pct": spam_pct,
        "nonspam_pct": nonspam_pct,

        # System Performance (users see this card)
        "uptime": _uptime_str(),
        "avg_response": "247ms",
        "memory_usage": "68%",
        "cpu_usage": "34%",

        # Model Performance (admins see this card)
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
        "trained_on": trained_on,
        # Also duplicate with model_* keys for template compatibility
        "model_accuracy": accuracy,
        "model_precision": precision,
        "model_recall": recall,
        "model_f1": f1,
        "model_trained_on": trained_on,
        "training_data": samples,

        # Chart.js payload (Dashboard)
        "chart_labels": ["Spam", "Non-Spam"],
        "chart_values": [int(spam_all or 0), int(not_spam_all or 0)],
    }

    return render_template(
        "dashboard.html",
        email=session.get("email"),
        **data
    )

# ---- Chart JSON for Spam vs Non-Spam (Dashboard & Reports) ----
@app.route("/api/spam_ratio")
def api_spam_ratio():
    if "email" not in session:
        return jsonify({"error": "not_authenticated"}), 401

    is_admin = (session.get("role") == "admin")
    scope = (request.args.get("scope") or "").lower()  # "all" if admin wants global
    if is_admin and scope == "all":
        spam_cnt, non_spam_cnt = _spam_nonspam_counts(limit_to_email=None)
    else:
        spam_cnt, non_spam_cnt = _spam_nonspam_counts(limit_to_email=session.get("email"))

    total = max(1, spam_cnt + non_spam_cnt)
    return jsonify({
        "labels": ["Spam", "Non-Spam"],
        "values": [spam_cnt, non_spam_cnt],
        "percents": {
            "spam": round(spam_cnt * 100 / total),
            "non_spam": 100 - round(spam_cnt * 100 / total)
        },
        "scope": "all" if (is_admin and scope == "all") else "user"
    })

# ---- Per-user ratio API (date-range aware, for Reports) ----
@app.get("/api/user_spam_ratio")
def api_user_spam_ratio():
    if "email" not in session or session.get("role") != "admin":
        return jsonify({"error": "forbidden"}), 403
    email_q = request.args.get("email", "ALL")
    start = request.args.get("start")
    end   = request.args.get("end")

    today = date.today()
    start_date = _parse_date(start, today - timedelta(days=6)) if start else (today - timedelta(days=6))
    end_date   = _parse_date(end, today) if end else today

    user_email = None if email_q == "ALL" else email_q
    spam_cnt, not_cnt, _ = _range_counts(start_date, end_date, limit_to_email=user_email)
    total = max(1, spam_cnt + not_cnt)
    return jsonify({
        "labels": ["Spam", "Non-Spam"],
        "values": [spam_cnt, not_cnt],
        "percents": {
            "spam": round(spam_cnt * 100 / total),
            "non_spam": 100 - round(spam_cnt * 100 / total)
        }
    })

# -------- Gmail helpers --------
def _gmail_fetch(service, page_token=None, mailbox="INBOX"):
    """Return (messages, next_token). mailbox=INBOX|SPAM|ALL"""
    mailbox = (mailbox or "INBOX").upper()
    list_kwargs = dict(userId="me", maxResults=10, pageToken=page_token, includeSpamTrash=True)
    if mailbox == "INBOX":
        list_kwargs["labelIds"] = ["INBOX"]
    elif mailbox == "SPAM":
        list_kwargs["labelIds"] = ["SPAM"]
    results = service.users().messages().list(**list_kwargs).execute()
    return results.get("messages", []), results.get("nextPageToken")

def _apply_server_side_cls_filter(email_list, cls_value):
    """Filter list by normalized class if cls_value provided."""
    cls_value = (cls_value or "").lower().replace("-", "_")
    if cls_value == "phishing":
        cls_value = "spam"  # backward compat
    if cls_value not in {"spam", "not_spam"}:
        return email_list

    def to_binary(label):
        return "spam" if _norm_label_raw(label) == "spam" else "not_spam"

    return [e for e in email_list if to_binary(e.get("classification")) == cls_value]

def _refresh_pull(service, label_id=None, max_results=25):
    """Pull a small batch from Gmail, classify & save; returns processed count."""
    kwargs = dict(userId="me", maxResults=max_results, includeSpamTrash=True)
    if label_id:
        kwargs["labelIds"] = [label_id]
    results = service.users().messages().list(**kwargs).execute()
    msgs = results.get("messages", []) or []
    n = 0
    for msg in msgs:
        md = service.users().messages().get(userId="me", id=msg["id"]).execute()
        hdrs = md.get("payload", {}).get("headers", [])
        subject = next((h["value"] for h in hdrs if h["name"] == "Subject"), "(No Subject)")
        sender  = next((h["value"] for h in hdrs if h["name"] == "From"), "(No Sender)")
        snippet = md.get("snippet", "") or ""
        label = classify_email(subject=subject, snippet=snippet)
        save_filtered_email(session.get("email"), msg["id"], sender, subject, snippet, label)
        n += 1
    return n

# -------- BULK classify entire mailbox (paginated) --------
def _bulk_refresh(service, scope='ALL', cap=5000, batch_size=100):
    """
    Paginate through Gmail and classify everything into the DB.
    - scope: 'ALL' | 'INBOX' | 'SPAM'
    - cap:   safety limit for how many to process in this run
    - batch_size: Gmail list page size (<=100)
    """
    scope = (scope or 'ALL').upper()
    label_ids = None
    if scope == 'INBOX':
        label_ids = ['INBOX']
    elif scope == 'SPAM':
        label_ids = ['SPAM']

    processed = 0
    page_token = None
    while processed < cap:
        kwargs = dict(userId='me', maxResults=min(100, batch_size), includeSpamTrash=True)
        if page_token:
            kwargs['pageToken'] = page_token
        if label_ids:
            kwargs['labelIds'] = label_ids

        res = service.users().messages().list(**kwargs).execute()
        msgs = res.get('messages', []) or []
        if not msgs:
            break

        for m in msgs:
            md = service.users().messages().get(userId='me', id=m['id']).execute()
            hdrs = md.get('payload', {}).get('headers', [])
            subject = next((h['value'] for h in hdrs if h['name'] == 'Subject'), '(No Subject)')
            sender  = next((h['value'] for h in hdrs if h['name'] == 'From'), '(No Sender)')
            snippet = md.get('snippet', '') or ''
            label  = classify_email(subject=subject, snippet=snippet)
            save_filtered_email(session.get('email'), m['id'], sender, subject, snippet, label)

            processed += 1
            if processed >= cap:
                break

        page_token = res.get('nextPageToken')
        if not page_token:
            break

    return processed

@app.route("/refresh_all")
def refresh_all():
    if "email" not in session or "credentials" not in session:
        return redirect(url_for("login"))

    scope = request.args.get("scope", "ALL")      # ALL | INBOX | SPAM
    cap   = int(request.args.get("cap", 5000))
    batch = int(request.args.get("batch", 100))

    creds = Credentials(**session["credentials"])
    creds = refresh_google_token_if_needed(creds)
    if creds is None:
        return redirect(url_for("login"))

    from googleapiclient.discovery import build
    service = build("gmail", "v1", credentials=creds)

    n = _bulk_refresh(service, scope=scope, cap=cap, batch_size=batch)
    flash(f"Classified {n} messages from {scope}. Run again to continue (duplicates ignored).", "success")
    return redirect(url_for("dashboard"))

# -------- One-click refresh for small batches (dashboard button) --------
@app.route("/refresh_emails")
def refresh_emails():
    if "email" not in session or "credentials" not in session:
        return redirect(url_for("login"))
    creds = Credentials(**session["credentials"])
    creds = refresh_google_token_if_needed(creds)
    if creds is None:
        return redirect(url_for("login"))
    from googleapiclient.discovery import build
    service = build("gmail", "v1", credentials=creds)

    n_inbox = _refresh_pull(service, label_id="INBOX", max_results=25)
    n_spam  = _refresh_pull(service, label_id="SPAM",  max_results=25)
    flash(f"Fetched & classified {n_inbox+n_spam} messages (Inbox:{n_inbox}, Spam:{n_spam}).", "success")
    return redirect(url_for("dashboard"))

# ------------------- Gmail Fetch with Next/Prev & Save -------------------
@app.route("/fetch_emails")
def fetch_emails():
    if "email" not in session or "credentials" not in session:
        return redirect(url_for("login"))

    creds = Credentials(**session["credentials"])
    creds = refresh_google_token_if_needed(creds)
    if creds is None:
        return redirect(url_for("login"))

    from googleapiclient.discovery import build
    service = build("gmail", "v1", credentials=creds)

    mailbox = request.args.get("mailbox", "INBOX")
    cls_filter = request.args.get("cls")  # spam | not_spam | (legacy phishing)

    current_page = int(request.args.get("page", 1))
    key = f"page_tokens_{mailbox.upper()}"
    if key not in session:
        session[key] = {1: None}
    page_tokens = session[key]
    page_token = page_tokens.get(str(current_page))

    messages, next_token = _gmail_fetch(service, page_token=page_token, mailbox=mailbox)
    if next_token:
        page_tokens[str(current_page + 1)] = next_token
    session[key] = page_tokens
    prev_token = page_tokens.get(str(current_page - 1))

    profile_info = service.users().getProfile(userId="me").execute()
    total_messages = profile_info.get("messagesTotal", 0)
    total_pages = max(1, (total_messages + 9) // 10)

    email_list = []
    for msg in messages or []:
        md = service.users().messages().get(userId="me", id=msg["id"]).execute()
        hdrs = md.get("payload", {}).get('headers', [])
        subject = next((h["value"] for h in hdrs if h["name"] == "Subject"), "(No Subject)")
        sender  = next((h["value"] for h in hdrs if h["name"] == "From"), "(No Sender)")
        snippet = md.get("snippet", "") or ""
        classification = classify_email(subject=subject, snippet=snippet)

        save_filtered_email(
            user_email=session.get("email"),
            gmail_id=msg["id"],
            sender=sender,
            subject=subject,
            snippet=snippet,
            classification=classification
        )
        email_list.append({
            "from": sender,
            "subject": subject,
            "snippet": snippet,
            "classification": classification
        })

    email_list = _apply_server_side_cls_filter(email_list, cls_filter)

    return render_template(
        "filtered_emails.html",
        emails=email_list,
        role=session.get("role"),
        email=session.get("email"),
        next_token=next_token,
        prev_token=prev_token,
        current_page=current_page,
        total_pages=total_pages,
        counts={"spam": 0, "not_spam": 0},
        selected_cls="all",
        q=""
    )

# ------------------- DB-backed Filtered Emails (all-time, paginated) -------------------
@app.route("/emails")
def emails_history():
    """Browse emails already classified & stored by the app (only the current user's)."""
    if "email" not in session:
        return redirect(url_for("login"))

    PAGE_SIZE = 10
    cls = (request.args.get("cls") or "all").lower().replace("-", "_")
    if cls == "phishing":  # backward compat
        cls = "spam"
    q = (request.args.get("q") or "").strip()
    page = max(1, int(request.args.get("page", 1)))
    offset = (page - 1) * PAGE_SIZE

    con = db_connect()
    cur = con.cursor()

    # ‚úÖ Always limit to the signed-in user (including admins)
    where = ["user_email=?"]
    params = [session.get("email")]

    # Optional class filter (binary)
    if cls in ("spam", "not_spam"):
        if cls == "spam":
            where.append("LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing')")
        else:
            where.append("LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam')")

    # Optional search
    if q:
        where.append("(sender LIKE ? OR subject LIKE ? OR snippet LIKE ?)")
        like = f"%{q}%"
        params.extend([like, like, like])

    where_sql = f" WHERE {' AND '.join(where)}"

    # Count for pagination
    cur.execute(f"SELECT COUNT(*) FROM filtered_emails{where_sql}", params)
    total_rows = cur.fetchone()[0] or 0
    total_pages = max(1, (total_rows + PAGE_SIZE - 1) // PAGE_SIZE)

    # Page data
    cur.execute(f"""
        SELECT sender, subject, snippet, classification
        FROM filtered_emails
        {where_sql}
        ORDER BY id DESC
        LIMIT ? OFFSET ?
    """, params + [PAGE_SIZE, offset])
    rows = cur.fetchall()
    emails = [{"from": r[0], "subject": r[1], "snippet": r[2], "classification": r[3]} for r in rows]

    # Totals per class (for filter badges) ‚Äî scoped to THIS user
    cur.execute("""
        SELECT
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('spam','phishing') THEN 1 ELSE 0 END) AS spam_cnt,
          SUM(CASE WHEN LOWER(REPLACE(classification,'-','_')) IN ('not_spam','ham','non_spam','not spam') THEN 1 ELSE 0 END) AS not_spam_cnt
        FROM filtered_emails
        WHERE user_email=?
    """, (session.get("email"),))
    sc, nsc = cur.fetchone() or (0, 0)
    counts = {"spam": sc or 0, "not_spam": nsc or 0}

    con.close()
    return render_template(
        "filtered_emails.html",
        emails=emails,
        role=session.get("role"),
        email=session.get("email"),
        current_page=page,
        total_pages=total_pages,
        next_token=None,
        prev_token=None,
        selected_cls=cls,
        q=q,
        counts=counts
    )

# ------------------- Filtered Emails (compat redirect) -------------------
@app.route("/filtered_emails")
def filtered_emails():
    return redirect(url_for("emails_history"))

# ------------------- Reports (Dynamic Dashboard) -------------------
@app.route("/reports")
def reports():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    # ---- Date range + selected user
    start = request.args.get("start")
    end   = request.args.get("end")
    selected_user = request.args.get("user", "ALL")

    today = date.today()
    start_date = _parse_date(start, today - timedelta(days=6)) if start else (today - timedelta(days=6))
    end_date   = _parse_date(end, today) if end else today

    # Global totals in range
    spam_g, not_g, users = _range_counts(start_date, end_date, limit_to_email=None)

    total_emails = spam_g + not_g
    pct = lambda n, d: int((n * 100) / d) if d else 0
    # Default (ALL) percents
    spam_pct_all = pct(spam_g, total_emails)
    ham_pct_all  = pct(not_g, total_emails)

    # Donut values based on selection (ALL vs per-user)
    if selected_user == "ALL":
        sel_spam, sel_ham = spam_g, not_g
    else:
        sel_spam, sel_ham, _ = _range_counts(start_date, end_date, limit_to_email=selected_user)

    # Percentages for the selected scope (used by right-side stat cards)
    total_sel = sel_spam + sel_ham
    spam_pct_sel = pct(sel_spam, total_sel)
    ham_pct_sel  = 100 - spam_pct_sel

    summary = build_reports_summary()  # all-time system summary (used by CSV/Excel too)

    # üî• IMPORTANT: Provide the SAME keys used by Dashboard donut
    chart_labels = ["Spam", "Non-Spam"]
    chart_values = [sel_spam, sel_ham]

    return render_template(
        "reports.html",
        role=session.get("role"),
        email=session.get("email"),
        summary=summary,                       # system summary (all-time)
        start_date=start_date,
        end_date=end_date,
        users=users,
        selected_user=selected_user,

        # Range KPIs (for cards on the page)
        total_emails=total_emails,
        total_spam=spam_g,
        total_ham=not_g,
        total_phish=0,                         # collapsed
        spam_pct=spam_pct_all,
        ham_pct=ham_pct_all,
        phish_pct=0,

        # Donut init (DASHBOARD-COMPAT KEYS)
        chart_labels=chart_labels,
        chart_values=chart_values,

        # For the % chips beside the donut on Reports
        spam_count=sel_spam,
        ham_count=sel_ham,
        spam_pct_sel=spam_pct_sel,
        ham_pct_sel=ham_pct_sel
    )

# ------------------- Manage Users -------------------
@app.route("/manage_users", methods=["GET", "POST"])
def manage_users():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    conn = db_connect()
    c = conn.cursor()
    if request.method == "POST":
        email = request.form.get("email")
        role = request.form.get("role", "user")
        try:
            c.execute("INSERT INTO users (email, role) VALUES (?, ?)", (email, role))
            conn.commit()
        except sqlite3.IntegrityError:
            pass
    c.execute("SELECT id, email, role FROM users ORDER BY id DESC")
    users = c.fetchall()
    conn.close()
    return render_template("manage_users.html", role=session.get("role"), email=session.get("email"), users=users)

@app.route("/delete_user/<int:user_id>", methods=["POST"])
def delete_user(user_id):
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))
    conn = db_connect()
    c = conn.cursor()
    c.execute("DELETE FROM users WHERE id = ?", (user_id,))
    conn.commit()
    conn.close()
    return redirect(url_for("manage_users"))

# ------------------- Update Model -------------------
@app.route("/update_model")
def update_model():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    accuracy, last_trained, samples = "N/A", "N/A", 0
    if os.path.exists(INFO_PATH):
        with open(INFO_PATH, encoding="utf-8") as f:
            lines = f.read().splitlines()
            if len(lines) >= 3:
                accuracy = lines[0].split("=")[1].strip()
                last_trained = lines[1].split("=")[1].strip()
                samples = lines[2].split("=")[1].strip()

    if os.path.exists(MASTER_DATASET):
        try:
            samples = len(pd.read_csv(MASTER_DATASET, encoding="utf-8"))
        except UnicodeDecodeError:
            samples = len(pd.read_csv(MASTER_DATASET, encoding="latin-1"))

    return render_template("update_model.html", role=session.get("role"), email=session.get("email"),
                           accuracy=accuracy, last_trained=last_trained, training_data=f"{samples}")

@app.route("/train_model", methods=["POST"])
def train_model_route():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    dataset = request.files.get("dataset")
    if not dataset:
        flash("‚ö†Ô∏è Please upload a dataset file first.", "warning")
        return redirect(url_for("update_model"))

    upload_path = "uploaded_dataset.csv"
    dataset.save(upload_path)

    try:
        # Prefer UTF-8
        try:
            new_data = pd.read_csv(upload_path, encoding="utf-8")
        except UnicodeDecodeError:
            new_data = pd.read_csv(upload_path, encoding="latin-1")
        new_data.columns = new_data.columns.str.strip().str.lower()

        # Auto-detect likely text/label columns
        cols = [c.lower().strip() for c in new_data.columns]
        new_data.columns = cols
        detected_format = None
        if "message" in cols and "category" in cols:
            new_data = new_data.rename(columns={"message": "text", "category": "label"})
            detected_format = "Generic message dataset (message/category)"
        elif "email" in cols and "target" in cols:
            new_data = new_data.rename(columns={"email": "text", "target": "label"})
            detected_format = "Email classification dataset (email/target)"
        elif "content" in cols and "type" in cols:
            new_data = new_data.rename(columns={"content": "text", "type": "label"})
            detected_format = "Web content dataset (content/type)"
        elif "v2" in cols and "v1" in cols:
            new_data = new_data.rename(columns={"v2": "text", "v1": "label"})
            detected_format = "SMS Spam Collection dataset (v1/v2)"
        elif "body" in cols:
            new_data = new_data.rename(columns={"body": "text"})
            detected_format = "Body-based dataset (body/label)"
        elif "class" in cols:
            new_data = new_data.rename(columns={"class": "label"})
            detected_format = "Class-based dataset (text/class)"
        if detected_format:
            flash(f"‚úÖ Detected dataset format: {detected_format}", "info")

        if not all(col in new_data.columns for col in ["text", "label"]):
            flash("‚ùå Invalid dataset format ‚Äî must contain 'text' and 'label' columns.", "danger")
            return redirect(url_for("update_model"))

        # Collapse labels to binary before merge (handles uploaded phishing/ham/etc.)
        new_data = new_data[["text", "label"]].dropna()
        new_data["label"] = new_data["label"].map(_norm_label_raw)
        new_data = new_data.drop_duplicates(subset=["text"]).reset_index(drop=True)

        # Merge into master (UTF-8)
        if os.path.exists(MASTER_DATASET):
            try:
                existing = pd.read_csv(MASTER_DATASET, encoding="utf-8")
            except UnicodeDecodeError:
                existing = pd.read_csv(MASTER_DATASET, encoding="latin-1")
            existing.columns = existing.columns.str.strip().str.lower()
            if "target" in existing.columns:
                existing = existing.rename(columns={"target": "label"})
            existing = existing[["text", "label"]].dropna()
            existing["label"] = existing["label"].map(_norm_label_raw)
            existing = existing.drop_duplicates(subset=["text"])

            combined = pd.concat([existing, new_data], ignore_index=True)
            combined = combined.drop_duplicates(subset=["text"]).reset_index(drop=True)
            combined.to_csv(MASTER_DATASET, index=False, encoding="utf-8")
            flash(f"üì¶ Dataset merged successfully ‚Äî total {len(combined)} rows.", "info")
        else:
            combined = new_data.copy()
            combined.to_csv(MASTER_DATASET, index=False, encoding="utf-8")
            flash(f"üì¶ New dataset saved ‚Äî {len(new_data)} rows.", "info")

        # --- LOG upload for Dataset Logs page ---
        try:
            uploaded_name = getattr(dataset, "filename", "") or "uploaded_dataset.csv"
            append_dataset_log(
                filename=uploaded_name,
                rows=len(new_data),
                cols=len(new_data.columns),
                detected_format=detected_format
            )
        except Exception:
            pass

        # Retrain
        import train_model
        train_model.train_model(MASTER_DATASET)

        # Reload model live
        global model, vectorizer
        model = joblib.load(MODEL_PATH)
        vectorizer = joblib.load(VEC_PATH)

        flash("‚úÖ Model retrained successfully with merged datasets!", "success")

    except Exception as e:
        flash(f"‚ùå Error during merging or training: {str(e)}", "danger")

    finally:
        if os.path.exists(upload_path):
            os.remove(upload_path)

    return redirect(url_for("update_model"))

# ------------------- Dataset Logs (paginated, newest first) -------------------
@app.route("/dataset_logs")
def dataset_logs():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    per_page = 10
    page = request.args.get("page", 1, type=int)
    if page < 1:
        page = 1

    if not os.path.exists(DATASET_LOG_PATH):
        resp = make_response(render_template(
            "dataset_logs.html",
            role=session.get("role"), email=session.get("email"),
            logs=[], page=1, pages=1, per_page=per_page, total=0
        ))
        resp.headers["Cache-Control"] = "no-store"
        return resp

    try:
        df = pd.read_csv(DATASET_LOG_PATH, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(DATASET_LOG_PATH, encoding="latin-1")

    df.columns = [c.lower() for c in df.columns]
    if "uploaded_at" in df.columns and "timestamp" not in df.columns:
        df = df.rename(columns={"uploaded_at": "timestamp"})

    if "timestamp" in df.columns:
        df = df.sort_values(by="timestamp", ascending=False)
    else:
        df = df.sort_index(ascending=False)

    total = len(df)
    pages = max(1, (total + per_page - 1) // per_page)
    start = (page - 1) * per_page
    end = start + per_page
    page_df = df.iloc[start:end].copy()

    for key in ["filename", "rows", "columns", "detected_format", "timestamp"]:
        if key not in page_df.columns:
            page_df[key] = ""

    logs = page_df[["filename", "rows", "columns", "detected_format", "timestamp"]].to_dict(orient="records")

    resp = make_response(render_template(
        "dataset_logs.html",
        role=session.get("role"), email=session.get("email"),
        logs=logs, page=page, pages=pages, per_page=per_page, total=total
    ))
    resp.headers["Cache-Control"] = "no-store"
    return resp

# ------------------- Model History API -------------------
@app.route("/model_history")
def model_history():
    if not os.path.exists(HISTORY_PATH):
        return {"timestamps": [], "accuracies": []}
    df = pd.read_csv(HISTORY_PATH)
    return {"timestamps": df["timestamp"].tolist(), "accuracies": df["accuracy"].tolist()}

# ------------------- CSV Download (robust, Excel-safe) -------------------
@app.route("/download_csv")
def download_csv():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    s = build_reports_summary()
    now = datetime.now()

    total = max(1, s["spam_detected"] + s["non_spam"])
    spam_pct = (s["spam_detected"] / total) * 100.0
    non_pct  = 100.0 - spam_pct

    # Add ?plain=1 to drop emoji headers (older Excel-friendly)
    use_emoji = str(request.args.get("plain", "")).lower() not in {"1", "true", "yes"}

    def pct_fmt(v):
        try:
            return f"{float(v):.2f}%"
        except Exception:
            return str(v)

    def bar(p, width=20):
        filled = int(round(p / 100.0 * width))
        return "‚ñà" * filled + "‚ñë" * (width - filled)

    if use_emoji:
        header = [
            "üìÖ Date", "‚è∞ Time",
            "üì® Emails Processed", "üö´ Spam Detected", "‚úÖ Non-Spam",
            "üî¥ Spam %", "üîµ Non-Spam %",
            "Spam Bar", "Non-Spam Bar",
            "üéØ Accuracy", "üéØ Precision", "üéØ Recall", "üéØ F1 Score",
            "üõ† Last Trained", "‚è± Uptime", "‚ö° Avg Response",
            "üß† Memory Usage", "üñ• CPU Usage",
            "üë• Active Users", "üö© Emails Flagged",
            "‚ùó False Positives", "‚ùó False Negatives", "üìö Training Sessions"
        ]
    else:
        header = [
            "Date", "Time",
            "Emails Processed", "Spam Detected", "Non-Spam",
            "Spam %", "Non-Spam %",
            "Spam Bar", "Non-Spam Bar",
            "Accuracy", "Precision", "Recall", "F1 Score",
            "Last Trained", "Uptime", "Avg Response",
            "Memory Usage", "CPU Usage",
            "Active Users", "Emails Flagged",
            "False Positives", "False Negatives", "Training Sessions"
        ]

    row = [
        now.strftime("%Y-%m-%d"), now.strftime("%H:%M"),
        s["emails_processed"], s["spam_detected"], s["non_spam"],
        f"{spam_pct:.2f}%", f"{non_pct:.2f}%",
        f"{bar(spam_pct)}  {spam_pct:.1f}%", f"{bar(non_pct)}  {non_pct:.1f}%",
        pct_fmt(s["accuracy"]), pct_fmt(s["precision"]), pct_fmt(s["recall"]), pct_fmt(s["f1"]),
        s["trained_on"], s["uptime"], s["avg_response"],
        s["memory_usage"], s["cpu_usage"],
        s["active_users"], s["emails_flagged"],
        s["false_positives"], s["false_negatives"], s["training_sessions"],
    ]

    # Build CSV in text buffer, then encode to bytes with UTF-8 BOM (Excel-friendly)
    txt_buf = StringIO(newline="")
    writer = csv.writer(txt_buf)
    writer.writerow(header)
    writer.writerow(row)
    csv_bytes = txt_buf.getvalue().encode("utf-8-sig")

    bio = BytesIO(csv_bytes)
    filename = f"report-{now.strftime('%Y%m%d-%H%M%S')}.csv"
    return send_file(
        bio,
        mimetype="text/csv",
        as_attachment=True,
        download_name=filename,
        max_age=0,
        conditional=False
    )

# ------------------- Excel Download (styled + chart) -------------------
@app.route("/download_report_xlsx")
def download_report_xlsx():
    if "email" not in session or session.get("role") != "admin":
        return redirect(url_for("dashboard"))

    s = build_reports_summary()
    total = max(1, s["spam_detected"] + s["non_spam"])
    spam_pct = (s["spam_detected"] / total) * 100.0
    non_pct  = 100.0 - spam_pct

    # Build dataframes
    summary_rows = [
        ("Generated At", datetime.now().strftime("%Y-%m-%d %H:%M:%S")),
        ("Emails Processed", s["emails_processed"]),
        ("Spam Detected", s["spam_detected"]),
        ("Non-Spam", s["non_spam"]),
        ("Spam %", f"{spam_pct:.2f}%"),
        ("Non-Spam %", f"{non_pct:.2f}%"),
        ("Accuracy", f"{s['accuracy']}%"),
        ("Precision", f"{s['precision']}%"),
        ("Recall", f"{s['recall']}%"),
        ("F1 Score", f"{s['f1']}%"),
        ("Last Trained", s["trained_on"]),
        ("Uptime", s["uptime"]),
        ("Avg Response", s["avg_response"]),
        ("Memory Usage", s["memory_usage"]),
        ("CPU Usage", s["cpu_usage"]),
        ("Active Users", s["active_users"]),
        ("Emails Flagged", s["emails_flagged"]),
        ("False Positives", s["false_positives"]),
        ("False Negatives", s["false_negatives"]),
        ("Training Sessions", s["training_sessions"]),
    ]
    df_summary = pd.DataFrame(summary_rows, columns=["Metric", "Value"])
    df_dist = pd.DataFrame(
        {"Class": ["Spam", "Non-Spam"], "Count": [s["spam_detected"], s["non_spam"]]}
    )

    output = io.BytesIO()
    try:
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            df_summary.to_excel(writer, sheet_name="Summary", index=False)
            df_dist.to_excel(writer, sheet_name="Distribution", index=False)

            wb  = writer.book
            ws1 = writer.sheets["Summary"]
            ws2 = writer.sheets["Distribution"]

            header_fmt = wb.add_format({"bold": True, "bg_color": "#F2F4F7", "border": 1})
            for col, width in zip(["A","B"], [26, 36]):
                ws1.set_column(f"{col}:{col}", width)
            for col, width in zip(["A","B"], [16, 14]):
                ws2.set_column(f"{col}:{col}", width)

            ws1.set_row(0, None, header_fmt)
            ws2.set_row(0, None, header_fmt)

            chart = wb.add_chart({"type": "doughnut"})
            chart.add_series({
                "name": "Spam vs Non-Spam",
                "categories": "=Distribution!$A$2:$A$3",
                "values":     "=Distribution!$B$2:$B$3",
                "data_labels": {"percentage": True},
            })
            chart.set_title({"name": "Spam vs Non-Spam"})
            ws2.insert_chart("D2", chart, {"x_scale": 1.2, "y_scale": 1.2})
    except Exception:
        with pd.ExcelWriter(output, engine="openpyxl") as writer:
            df_summary.to_excel(writer, sheet_name="Summary", index=False)
            df_dist.to_excel(writer, sheet_name="Distribution", index=False)

    output.seek(0)
    filename = f"report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.xlsx"
    return send_file(
        output,
        as_attachment=True,
        download_name=filename,
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )

# ------------------- Logout -------------------
@app.route("/logout")
def logout():
    session.clear()
    flash("üëã You‚Äôve been logged out successfully.", "info")
    return redirect(url_for("index"))

# ------------------- Run App -------------------
if __name__ == "__main__":
    app.run(debug=True)
